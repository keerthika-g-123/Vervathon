{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4Y0ZKPKctV23"
      },
      "outputs": [],
      "source": [
        "%%capture reqirments\n",
        "!pip install langchain\n",
        "!pip install langchain-community\n",
        "!pip install py-localtunnel\n",
        "!npm install -g localtunnel\n",
        "!pip install streamlit\n",
        "!pip install langchain-experimental\n",
        "!pip install langchain_hugginpipgface\n",
        "!pip install huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AYwDn13E5iu8",
        "outputId": "2974ed2f-2a0a-407f-a58e-07c0f573a29c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.agents import AgentType\n",
        "import streamlit as st\n",
        "import os\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]=\"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\"\n",
        "API=\"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\"\n",
        "\n",
        "st.set_page_config(\n",
        "    page_title=\"Large Language Model - ChatBot\",\n",
        "    page_icon=\"ðŸ’»\",\n",
        "    layout=\"centered\"\n",
        ")\n",
        "\n",
        "st.title(\"Large Language Model ðŸ’»ðŸ”— Text-Generation\")\n",
        "\n",
        "\n",
        "if \"chat_history\" not in st.session_state:\n",
        "  st.session_state.chat_history=[]\n",
        "\n",
        "for message in st.session_state.chat_history:\n",
        "  with st.chat_message(message[\"role\"]):\n",
        "    st.markdown(message[\"content\"])\n",
        "\n",
        "user_prompt=st.chat_input(\"Ask Large Language Model\")\n",
        "\n",
        "if user_prompt:\n",
        "  st.chat_message(\"user\").markdown(user_prompt)\n",
        "  st.session_state.chat_history.append({\"role\":\"user\",\"content\":user_prompt})\n",
        "  LargeLanguageModel=HuggingFaceEndpoint(repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\",max_length=200,temperature=0.3,token=API)\n",
        "\n",
        "  response=LargeLanguageModel.invoke(user_prompt)\n",
        "\n",
        "  st.session_state.chat_history.append({\"role\":\"assistant\",\"content\":response})\n",
        "\n",
        "  with st.chat_message(\"assistant\"):\n",
        "    st.markdown(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_Yqeh_tOrIU",
        "outputId": "6f1abcb6-f7c9-4b20-930e-914375933762"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "34.141.174.9\n"
          ]
        }
      ],
      "source": [
        "! wget -q -O - ipv4.icanhazip.com"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9LXstYlOtZM",
        "outputId": "9805c4e8-ba09-4fd0-9cf8-fc59aae1b8f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.141.174.9:8501\u001b[0m\n",
            "\u001b[0m\n",
            "your url is: https://plain-dots-fetch.loca.lt\n",
            "WARNING! max_length is not default parameter.\n",
            "                    max_length was transferred to model_kwargs.\n",
            "                    Please make sure that max_length is what you intended.\n",
            "WARNING! token is not default parameter.\n",
            "                    token was transferred to model_kwargs.\n",
            "                    Please make sure that token is what you intended.\n",
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: fineGrained).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "! streamlit run app.py & npx localtunnel --port 8501"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
